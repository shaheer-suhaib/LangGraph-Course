{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69371036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEYY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "joke_model =ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ").with_config({\"tags\": [\"joke\"]})\n",
    "\n",
    "poem_model = llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ").with_config({\"tags\": [\"poem\"]})\n",
    "\n",
    "# The tags are not tied to different APIs. They are just labels to distinguish outputs during streaming.\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "      topic: str\n",
    "      joke: str\n",
    "      poem: str\n",
    "\n",
    "\n",
    "async def call_model(state, config):\n",
    "      topic = state[\"topic\"]\n",
    "      print(\"Writing joke...\")\n",
    "      # Note: Passing the config through explicitly is required for python < 3.11\n",
    "      # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n",
    "      \n",
    "      joke_response = await joke_model.ainvoke(\n",
    "            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n",
    "            config, \n",
    "      )\n",
    "      print(\"\\n\\nWriting poem...\")\n",
    "      poem_response = await poem_model.ainvoke(\n",
    "            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n",
    "            config, \n",
    "      )\n",
    "      return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n",
    "\n",
    "\n",
    "graph = (\n",
    "      StateGraph(State)\n",
    "      .add_node(call_model)\n",
    "      .add_edge(START, \"call_model\")\n",
    "      .compile()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b475ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing joke...\n",
      "Why| was the cat sitting on the computer?  To keep an eye on the mouse|!\n",
      "|\n",
      "\n",
      "Writing poem...\n"
     ]
    }
   ],
   "source": [
    "async for msg, metadata in graph.astream(\n",
    "      {\"topic\": \"cats\"},\n",
    "      stream_mode=\"messages\", \n",
    "):\n",
    "    if metadata[\"tags\"] == [\"joke\"]: \n",
    "        print(msg.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed95f8",
   "metadata": {},
   "source": [
    "# âœ… Why is async used in this code?\n",
    "so you said that it is used when in one node mulltiple llms calls are to be called.\n",
    "ðŸ§  Why?\n",
    "If you donâ€™t use async, then:\n",
    "\n",
    "Joke LLM will finish\n",
    "\n",
    "Then Poem LLM will start\n",
    "\n",
    "= â³ Slower\n",
    "\n",
    "With async and await:\n",
    "\n",
    "Both run at the same time\n",
    "\n",
    "= âš¡ Faster response\n",
    "\n",
    "# so let say if we have many nodes in grpah.\n",
    "and each graph has one llms call(and response needed to stream to user from that specific node).\n",
    "so in this case  i dont need async ?\n",
    "am i right? and why "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c88101",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47ed29fe",
   "metadata": {},
   "source": [
    "â—Why \"Filter by LLM Invocation\" is Still Useful\n",
    "Even though nodes run one-by-one:\n",
    "\n",
    "Streaming happens token-by-token inside each node\n",
    "\n",
    "If a node itself uses multiple LLMs or tools in parallel (with async)\n",
    "\n",
    "Or if you want to build a real-time UI where you stream only part of the graph output\n",
    "\n",
    "Or if you're debugging logs and only care about one step\n",
    "\n",
    "Or you have shared components (like same LLM used in multiple places), tagging helps isolate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16ee04",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'â—' (U+2757) (3280518969.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    â—Why \"Filter by LLM Invocation\" is Still Useful\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character 'â—' (U+2757)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8804160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
