{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d57eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "load_dotenv()\n",
    "# model = genai.GenerativeModel('gemini-1.5-flash') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40bc9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",google_api_key=  GEMINI_API_KEY ,disable_streaming=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a93cafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000211549DB5F0>, default_metadata=(), model_kwargs={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4b148b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async Invoke Result: input_variables=[] input_types={} partial_variables={} messages=[AIMessage(content=\"The sky is blue due to a phenomenon called **Rayleigh scattering**.  Sunlight is made up of all the colors of the rainbow.  As sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen and oxygen).  These molecules are much smaller than the wavelengths of visible light.\\n\\nRayleigh scattering affects shorter wavelengths of light more strongly than longer wavelengths.  Blue and violet light have the shortest wavelengths, so they are scattered more effectively by the air molecules in all directions.  This scattered blue light is what we see when we look at the sky.\\n\\nWhile violet light is scattered even more than blue light, our eyes are less sensitive to violet, and the sun emits slightly less violet light, resulting in a blue sky rather than a violet one.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--301e3bdc-a33e-40c3-bc23-fa017da1f831-0', usage_metadata={'input_tokens': 6, 'output_tokens': 159, 'total_tokens': 165, 'input_token_details': {'cache_read': 0}}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='...'), additional_kwargs={})]\n",
      "\n",
      "Async Stream Result:\n",
      "The thread unwinds, a task begun,\n",
      "No waiting here, the race is run.\n",
      "A promise made, a future bright,\n",
      "While other tasks take up the light.\n",
      "\n",
      "Callbacks chime, a gentle song,\n",
      "When work is done, where it belongs.\n",
      "No blocking waits, no idle spin,\n",
      "Asynchronous, from within.\n",
      "\n",
      "\n",
      "Async Batch Results: ['1 + 1 = 2', '2 + 2 = 4']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async def run_async_calls():\n",
    "    # Async invoke\n",
    "    result_ainvoke = await llm.ainvoke(\"Why is the sky blue?\")\n",
    "    print(\"Async Invoke Result:\", result_ainvoke + \"...\")\n",
    "\n",
    "    # Async stream\n",
    "    print(\"\\nAsync Stream Result:\")\n",
    "    async for chunk in llm.astream(\n",
    "        \"Write a short poem about asynchronous programming.\"\n",
    "    ):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Async batch\n",
    "    results_abatch = await llm.abatch([\"What is 1+1?\", \"What is 2+2?\"])\n",
    "    print(\"Async Batch Results:\", [res.content for res in results_abatch])\n",
    "\n",
    "\n",
    "await run_async_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9d7d064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model, trained by Google.  My existence is purely digital; I have no physical body or personal experiences.  I don't have feelings, opinions, or beliefs in the human sense.  My purpose is to process information and respond to prompts in a comprehensive and informative way.\n",
      "\n",
      "I learn from vast amounts of text data, absorbing patterns and relationships between words and concepts.  This allows me to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way, even if they are open ended, challenging, or strange.\n",
      "\n",
      "I strive for accuracy and neutrality in my responses, avoiding biases present in my training data as much as possible.  I am constantly evolving and improving through ongoing updates and refinements to my algorithms.  My knowledge cutoff is a specific point in time, so my information may not be completely up-to-date on very recent events.\n",
      "\n",
      "I am a tool, a resource, designed to assist users with a wide range of tasks.  I can help with research, writing, translation, and even creative endeavors like storytelling or poetry.  My capabilities are broad, but my limitations are inherent in my design.  I cannot provide personal advice, medical or legal counsel, or engage in actions in the real world.\n",
      "\n",
      "I am a complex system, a product of advanced artificial intelligence research.  My development is an ongoing process, pushing the boundaries of what's possible in natural language processing.  I am a work in progress, constantly learning and adapting to better serve my users.  I hope to be a helpful and informative resource for you.  Ask me anything!\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\n",
    "        \"Write about you in 50 lines.\"\n",
    "    ):\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0dd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
